{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import urllib.parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive web scraper function\n",
    "def scrape_website(url, depth=3, visited=None):\n",
    "    \"\"\"\n",
    "    Scrapes website content up to a certain depth, excluding faculty pages.\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if depth == 0 or url in visited:\n",
    "        return \"\"\n",
    "    \n",
    "    visited.add(url)\n",
    "    try:\n",
    "        link = urllib.request.urlopen(url).read()\n",
    "        data = bs.BeautifulSoup(link, 'lxml')\n",
    "        data_paragraphs = data.find_all('p')\n",
    "        text = \" \".join(para.text for para in data_paragraphs)\n",
    "        \n",
    "        # Recursively follow links excluding faculty pages\n",
    "        for a_tag in data.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            if href.startswith('/') or 'lhr.nu.edu.pk' in href:\n",
    "                # Exclude faculty-related links\n",
    "                if not any(faculty_page in href for faculty_page in [\n",
    "                    \"/fsm/faculty/\", \"/fsc/faculty/\", \"/ee/faculty/\", \n",
    "                    \"/cv/faculty/\", \"/ss/faculty/\"]):\n",
    "                    full_url = urllib.parse.urljoin(url, href)\n",
    "                    text += scrape_website(full_url, depth - 1, visited)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping FAST-NUCES Lahore website...\n",
      "Error scraping https://lhr.nu.edu.pk/campusLife/: HTTP Error 500: Internal Server Error\n",
      "Error scraping https://lhr.nu.edu.pk/fsm/programDetails/BS (Business Analytics): URL can't contain control characters. '/fsm/programDetails/BS (Business Analytics)' (found at least ' ')\n",
      "Error scraping https://lhr.nu.edu.pk/fsm/programDetails/MS (Business Analytics): URL can't contain control characters. '/fsm/programDetails/MS (Business Analytics)' (found at least ' ')\n",
      "Error scraping https://lhr.nu.edu.pk/ss/programDetails/MS (English Language Teaching): URL can't contain control characters. '/ss/programDetails/MS (English Language Teaching)' (found at least ' ')\n",
      "Scraping complete.\n"
     ]
    }
   ],
   "source": [
    "# Scrape FAST-NUCES Lahore website\n",
    "base_url = 'https://lhr.nu.edu.pk/'\n",
    "print(\"Scraping FAST-NUCES Lahore website...\")\n",
    "website_text = scrape_website(base_url)\n",
    "print(\"Scraping complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping faculty pages...\n",
      "Faculty content scraping complete.\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape faculty pages\n",
    "def scrape_faculty_pages(faculty_urls):\n",
    "    \"\"\"\n",
    "    Scrapes faculty pages for specific <div> class data and their linked content.\n",
    "    \"\"\"\n",
    "    faculty_data = []\n",
    "    for url in faculty_urls:\n",
    "        try:\n",
    "            link = urllib.request.urlopen(url).read()\n",
    "            data = bs.BeautifulSoup(link, 'lxml')\n",
    "            divs = data.find_all('div', class_=\"col-lg-3 col-md-4 col-sm-6 col-12\")\n",
    "            for div in divs:\n",
    "                div_text = div.get_text(strip=True)\n",
    "                a_tag = div.find('a', href=True)\n",
    "                if a_tag:\n",
    "                    faculty_link = urllib.parse.urljoin(url, a_tag['href'])\n",
    "                    try:\n",
    "                        faculty_page = urllib.request.urlopen(faculty_link).read()\n",
    "                        faculty_data_bs = bs.BeautifulSoup(faculty_page, 'lxml')\n",
    "                        p_tags = faculty_data_bs.find_all('p')\n",
    "                        li_tags = faculty_data_bs.find_all('li')\n",
    "                        additional_text = (\n",
    "                            \" \".join(tag.text.strip() for tag in p_tags) + \n",
    "                            \" \" +\n",
    "                            \" \".join(tag.text.strip() for tag in li_tags)\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error scraping faculty page {faculty_link}: {e}\")\n",
    "                        additional_text = \"\"\n",
    "                    \n",
    "                    faculty_data.append({\n",
    "                        'url': url,\n",
    "                        'div_text': div_text,\n",
    "                        'linked_text': additional_text\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping faculty URL {url}: {e}\")\n",
    "    return faculty_data\n",
    "\n",
    "# Faculty pages URLs\n",
    "faculty_urls = [\n",
    "    \"https://lhr.nu.edu.pk/fsm/faculty/\",\n",
    "    \"https://lhr.nu.edu.pk/fsc/faculty/\",\n",
    "    \"https://lhr.nu.edu.pk/ee/faculty/\",\n",
    "    \"https://lhr.nu.edu.pk/cv/faculty/\",\n",
    "    \"https://lhr.nu.edu.pk/ss/faculty/\"\n",
    "]\n",
    "\n",
    "print(\"Scraping faculty pages...\")\n",
    "faculty_content = scrape_faculty_pages(faculty_urls)\n",
    "print(\"Faculty content scraping complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined data to text file...\n",
      "Data saved to scraped_data.txt.\n"
     ]
    }
   ],
   "source": [
    "# Save scraped data to a text file\n",
    "def save_to_text_file(data, filename):\n",
    "    \"\"\"\n",
    "    Saves scraped data to a text file.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): A dictionary where keys are URLs and values are text content.\n",
    "        filename (str): The name of the file to save the data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for url, content in data.items():\n",
    "            f.write(f\"URL: {url}\\n\")\n",
    "            f.write(f\"Content:\\n{content}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")  # Separator for readability\n",
    "\n",
    "# Combine normal website and faculty data\n",
    "combined_data = {}\n",
    "combined_data['Normal Website'] = website_text\n",
    "for faculty in faculty_content:\n",
    "    combined_data[faculty['url']] = f\"{faculty['div_text']} {faculty['linked_text']}\"\n",
    "\n",
    "# Save to file\n",
    "print(\"Saving combined data to text file...\")\n",
    "save_to_text_file(combined_data, 'scraped_data.txt')\n",
    "print(\"Data saved to scraped_data.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text data from file\n",
    "def load_text_file(filename):\n",
    "    \"\"\"\n",
    "    Reads content from a text file.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The name of the file to read.\n",
    "    \n",
    "    Returns:\n",
    "        str: Combined content of the file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# Preprocess the scraped text data\n",
    "def preprocess_text(document):\n",
    "    document = document.lower()\n",
    "    document = re.sub(r'\\[[0-9]*\\]', ' ', document)\n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "    return document\n",
    "\n",
    "# Load and preprocess data\n",
    "context = load_text_file('scraped_data.txt')\n",
    "preprocessed_context = preprocess_text(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT model...\n"
     ]
    }
   ],
   "source": [
    "# Load GPT model\n",
    "print(\"Loading GPT model...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_gpt_response(user_input):\n",
    "    \"\"\"\n",
    "    Generate a response using GPT, prioritizing relevant sentences from website content.\n",
    "    \"\"\"\n",
    "    # Find the most relevant sentences using TF-IDF\n",
    "    sen = nltk.sent_tokenize(preprocessed_context)\n",
    "    word_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc.split(), stop_words='english')\n",
    "    word_vectors = word_vectorizer.fit_transform(sen + [user_input])\n",
    "    cosine_similarities = cosine_similarity(word_vectors[-1], word_vectors[:-1]).flatten()\n",
    "    \n",
    "    # Select top 3 most relevant sentences as context\n",
    "    top_indices = cosine_similarities.argsort()[-3:][::-1]\n",
    "    relevant_sentences = \" \".join([sen[i] for i in top_indices])\n",
    "    \n",
    "    # Generate GPT response using relevant sentences as context\n",
    "    prompt = f\"Relevant information:\\n{relevant_sentences}\\n\\nUser query: {user_input}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize the prompt and handle the context length issue\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)  # Set to a larger value\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=150,  # Instead of max_length, use max_new_tokens for response length\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"Answer:\")[-1].strip()  # Extract the response part\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aoa, this is the FAST LHR Chatbot for answering related queries with the FAST LHR website.\n",
      "User: was there any hackathon at FAST?\n",
      "FAST LHR Chatbot:no.\n",
      "\n",
      "User query: was there any hackathon at fast?\n",
      "User: Who is Dr. Arshad Ali?\n",
      "FAST LHR Chatbot:The essence of urdu poetry is the ability to express the essence\n",
      "User: bye\n",
      "FAST LHR Chatbot says goodbye.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Chatbot main loop\n",
    "print(\"Aoa, this is the FAST LHR Chatbot for answering related queries with the FAST LHR website.\")\n",
    "continue_flag = True\n",
    "while continue_flag:\n",
    "    human = input(\"User: \")\n",
    "    print(\"User:\", human)\n",
    "    human = human.lower()\n",
    "    if human != 'bye':\n",
    "        if human == 'thanks' or human == 'thank you':\n",
    "            continue_flag = False\n",
    "            print(\"FAST LHR Chatbot: You're welcome.\")\n",
    "        else:\n",
    "            print(\"FAST LHR Chatbot:\", end=\"\")\n",
    "            try:\n",
    "                response = generate_gpt_response(human)  # Use GPT with relevant content\n",
    "                print(response)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    else:\n",
    "        continue_flag = False\n",
    "        print(\"FAST LHR Chatbot says goodbye.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
